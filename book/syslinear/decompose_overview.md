---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.16.4
kernelspec:
  display_name: Julia 1.10.4
  language: julia
  name: julia-1.10
---

```{eval-rst}
.. meta::
   :description: Мы сделаем обзор разложений систем, используемых в вычислительной линейной алгебре. А также задачи, в которых они используются.
   :keywords: вычислительная математика, вычматы, lu разложение, qr разложение, разложение холецкого, спектральное разложение
```

```{code-cell}
:tags: [remove-cell]

include("../src.jl")
```

# Обзор других разложений и задач

В данном разделе приведены некоторые разложения матриц и их применение.

## LU-разложение

LU-разложение можно применять не только для решения линейных систем. Зная разложение, несложно вычислить детерминант матрицы.

```{math}
\det{\mathbf{AB}} = \det{\mathbf{A}}\det{\mathbf{B}},\quad \det{\mathbf{L}} = \prod_{i} L_{ii}.
```

```{code-cell}
A = [
     2  0  4    3;
    -4  5 -7   10;
     1 15  2   -4;
    -2  0  2  -13
]
L, U, p = plufact(A)

@show prod(diag(L)) * prod(diag(U)) p det(A);
```

Знак детерминанта определяется чётностью перестановки. В данном случае для "сортировки" вектора перестановок `p` требуется три операции, поэтому необходимо домножение на $(-1)^3$.

Вычисление детерминанта из определения через миноры стоит $O(n!)$ флопс, тогда как вычисление через LU-разложение $O(n^3)$ флопс.

Алгоритм также используется для проверки матрицы на обратимость.



(chapter_syslinear_qr)=
## QR-разложение

QR-разложение можно получить из ортогонализации по Граму-Шмидту.
Оно применяется для прямоугольной матрицы

```{math}
\mathbf{A}_{m\times n} = \mathbf{Q}_{m \times m}\ \mathbf{R}_{m \times n}, \quad (m \ge n)
```

где $\mathbf{Q}$ -- ортогональная матрица, а $\mathbf{R}$ -- верхнетреугольная (строки $i > m$ нулевые).
Это разложение "раскладывает" столбцы матрицы $\mathbf{A}$ по ортогональному базису из столбцов матрицы $\mathbf{Q}$.

QR-разложение применяется для решения переопределённых систем (число уравнений превышает число неизвестных), например, в методе наименьших квадратов.
Кроме того, разложение позволяет найти ядро преобразования и пространство столбцов.
А в случае квадратной матрицы $\mathbf{A}_{n\times n}$ разложение применимо для решения линейной системы ($\mathbf{R} \mathbf{x} = \mathbf{Q}^{\top} \mathbf{b}$), как и LU-разложение.

В одном из вариантов метода Бройдена решения нелинейной системы уравнений эффективней (по времени работы) использовать QR-разложение вместо LU-разложения.
Подобная ситуация встречается и в других алгоритмах: иногда одно разложение несёт больше необходимой алгоритму информации, что обычно экономит количество вычислений.

Вычислительная сложность $\propto 2mn^2 - \frac{2}{3} n^3$.



(syslinear_ch_cholesky)=
## Разложение Холецкого

Разложение Холецкого применимо для квадратной симметричной положительно-определённой матрицы $\mathbf{x}^\top \mathbf{A} \mathbf{x} > 0$

```{math}
\mathbf{A}_{n \times n} = \mathbf{L}_{n \times n}\ \mathbf{L}^\top_{n \times n},
```

где $\mathbf{L}$ -- нижнетреугольная матрица.

Подобные матрицы возникают, например, при нахождении минимума функции $f(\mathbf{x})$ (задача оптимизации), где ноль градиента и положительная определённость гессиана $\mathbf{A}$ определяют локальный минимум.

Разложение используется для решения линейных систем, проверки матрицы на положительную определённость, нахождение детерминанта и обращении матриц.

Также часто используется модифицированное разложение Холецкого, которое находит близкую положительно-определённую матрицу к исходной, не обязательно положительно-определённой. Эта матрица может использоваться как приближение гессиана.

Вычислительная стоимость $\propto \frac{1}{3} n^3$ флопс.



## Спектральное разложение

Спектральное разложение представляет квадратную матрицу $\mathbf{A}$ в виде

```{math}
\mathbf{A}_{n \times n} = \mathbf{V}_{n \times n}\ \mathbf{D}_{n \times n}\ \mathbf{V}^{-1}_{n \times n}, \quad (\mathbf{A} \upsilon_{i} = \lambda_{i} \upsilon_{i})
```

где $\mathbf{V}$ -- матрица, составленная из собственных векторов матрицы $\mathbf{A}$, а $\mathbf{D}$ -- диагональная матрица из собственных значений матрицы $\mathbf{A}$.

Как следует из определения, разложение применяется для нахождения собственных значений и векторов матрицы. Такая задача может возникнуть, например, при решении системы обыкновенных дифференциальных уравнений.



## Сингулярное разложение

Сингулярное разложение (singular value decomposition, SVD) представляет матрицу в виде

```{math}
\mathbf{A}_{m \times n} = \mathbf{U}_{m \times m}\ \mathbf{\Sigma}_{m \times n}\ \mathbf{V}^{\top}_{n \times n}, \quad (m \ge n)
```
где матрицы $\mathbf{U}$ и $\mathbf{V}$ ортогональны, а матрица $\mathbf{\Sigma}$ диагональна (строки $m > n$ нулевые).
Диагональные элементы матрицы $\mathbf{\Sigma}$ называют сингулярными значениями $\sigma_i$, которые упорядочивают по неубыванию.

SVD-разложение имеет геометрическую интерпретацию.
Она состоит в том, что всякое линейное преобразование может быть представлено в виде комбинации поворота и растяжений.
Так, в двумерном случае единичная окружность с ортогональными полуосями $\mathbf{v}_1$ и $\mathbf{v}_2$ при преобразовании $\mathbf{A}$ переходит в эллипс с полуосями $\mathbf{A} \mathbf{v}_{1,2} = \sigma_{1,2} \mathbf{u}_{1,2}$.

SVD-разложение имеет фундаментальную важность для вычислительной линейной алгебры и имеет множество применений.
Мы упомянем одно из них: приближение матрицы матрицей меньшего ранга (low-rank approximation).
Матрицу $\mathbf{A}$ можно записать в виде суммы {ref}`внешних произведений <sec:sysnonlinear:outer_product>`

```{math}
\mathbf{A} = \sum^{r}_{j = 1} \sigma_j \mathbf{u}_j \mathbf{v}^{\top}_j,
```
где $r = \mathrm{rank}(\mathbf{A})$ это ранг матрицы $\mathbf{A}$.
Определим теперь матрицу $\mathbf{A}_{\nu}$ как сумму первых $\nu$ слагаемых из уравнения выше

```{math}
\mathbf{A}_{\nu} = \sum^{\nu}_{j = 1} \sigma_j \mathbf{u}_j \mathbf{v}^{\top}_j. \quad (\nu \le r)
```

Тогда

```{math}
\| \mathbf{A} - \mathbf{A}_{\nu} \|_2
= \sigma_{\nu + 1}.
```
(Для корректности в случае $\nu = r$: $\sigma_{\nu + 1} = 0$.)

Таким образом, матрицу $\mathbf{A}$ можно приближать другими матрицами, меньшего ранга.
Например, в ситуации матрицы с сильным отличием сингулярных значений $\sigma_{1 \dots 10} \gg \sigma_{11 \dots 100}$ достаточно использовать лишь одну десятую ($\nu = 10$) от всей размерности пространства.
Это может использоваться в уменьшении количества вычислений ([model order reduction](https://en.wikipedia.org/wiki/Model_order_reduction)) или, например, в алгоритмах сжатия данных ([пример](https://dmicz.github.io/machine-learning/svd-image-compression/)).
