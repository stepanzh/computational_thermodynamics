```{eval-rst}
.. meta::
   :description: Метод Ньютона с поиском вдоль направления для решение задачи оптимизации.
   :keywords: метод ньютона, разложение холецкого, поиск вдоль направления, line search, оптимизация, минимизация, вычислительная математика, вычматы
```

(sec:optim:newton)=
# Метод Ньютона

Приблизим локально значение функции $f$ в точке $\mathbf{x}_k + \mathbf{d}$ квадратичной функцией $m_k(\mathbf{d})$, используя разложение Тейлора

```{math}
f(\mathbf{x}_k + \mathbf{d})
\approx f(\mathbf{x}_k) + \nabla f(\mathbf{x}_k)^\top \mathbf{d}
  + \frac{1}{2} \mathbf{d}^\top \nabla^2 f(\mathbf{x}_k) \mathbf{d}
= m_k(\mathbf{d}).
```

Предположим, что гессиан $\nabla^2 f(\mathbf{x}_k)$ *положительно определён*, тогда в качестве направления убывания возьмём вектор $\mathbf{p}$, который минимизирует модельную функцию $m_k(\mathbf{d})$. Для этого найдём ноль градиента функции $m_k$

```{math}
\mathbf{0} = \nabla m_k(\mathbf{p}) = \nabla f(\mathbf{x}_k) + \nabla^2 f(\mathbf{x}_k) \mathbf{d},
```

откуда

```{math}
:label: optim_newton_dir

\mathbf{d} = - (\nabla^2 f(\mathbf{x}_k))^{-1} \nabla f(\mathbf{x}_k).
```

Данный вектор будем использовать в качестве направления убывания на $k$-ом шаге оптимизации. Направление вида {eq}`optim_newton_dir` называют *Ньютоновским*. В сочетании с линейным поиском получаем *метод Ньютона для задачи оптимизации*. Однако, прежде сделаем модификацию.

## Модификация метода

Отметим предположения, при которых {eq}`optim_newton_dir` справедливо. Во-первых, размер направления $\|\mathbf{d}\|$ не должен быть слишком большим, во-вторых, мы предполагали положительную определённость гессиана.

На практике размер шага $\|\mathbf{d}\|$ подбирается линейным поиском $\mathbf{x}_k + \alpha \mathbf{d}$, начиная с $\alpha = 1$. Вдали от минимума подходящие шаги могу оказаться $\alpha < 1$, однако, вблизи минимума полный шаг чаще подходит.

Направление $\mathbf{d}$ {eq}`optim_newton_dir` может быть не убывающим вдали от минимума, поскольку гессиан может быть не положительно определён. Чтобы гарантировать положительную определённость, используют *модифицированное разложение Холецкого*. Мы уже упоминали о {ref}`Разложении Холецкого <syslinear_ch_cholesky>`. Его же модифицированная версия принимает на вход произвольную матрицу $\mathbf{A}$ и, пытаясь выполнить стандартное разложение, корректирует элементы $\mathbf{A}$ так, чтобы итоговая матрица $\mathbf{A}^{(m)}$ была положительно определена и не слишком сильно отличалась от исходной:

```{math}
\mathbf{A} \approx \mathbf{A}^{(m)} = \mathbf{L}\mathbf{L}^\top.
```

Наша модификация алгоритма Ньютона заключается в использовании на $k$-ом шаге оптимизации не исходного гессиана $\mathbf{H}$, а его приближения $\mathbf{H}^{(m)}$, полученного из модифицированного разложения Холецкого. Вдали от минимума гессиан и приближение могут отличаться, однако вблизи минимума разложение практически не внесёт поправок.

В принципе, мы могли бы проверять на итерации, является ли гессиан положительно определённым и, если нет, то использовать $\mathbf{H}^{(m)}$. Однако, процедура проверки строится на попытке выполнить стандартное разложение Холецкого, поэтому большой выгоды от этого нет.

## Реализация

Модифицированное разложение Холецкого приведено в {ref}`Приложении <appendix_ch_mcholesky>`.

Ниже дан шаблон метода Ньютона, который нужно будет завершить в домашнем задании.

Структура данных для результата алгоритма

```julia
struct NewtonResult{T<:Real}
    converged::Bool      # метод сошёлся
    argument::Vector{T}  # найденный минимум
    iters::Int           # число прошедших итераций
end
```

```{proof:function} newton

**Метод Ньютона (шаблон)**

:::julia
"""
    newton(f, ∇f, hess, x0[; gtol=1e-5, maxiter=200])

Поиск локального безусловного минимума функции `f`, с градиентом ∇f и гессианом `hess`. Поиск начинается с `x0` и заканчивается, когда норма градиента не превышает `gtol` или максимальное число итераций `maxiter` превышено.

- `f::Function`: по вектору x возвращает значение функции, `f(x)`: `Vector` → `Real`;
- `∇f::Function`: по вектору x возвращает градиент `f`, `∇f(x)`: `Vector` → `Vector`;
- `hess::Function`: по вектору x возвращает гессиан `f`, `hess(x)`: `Vector` → `Matrix`.

Возвращает `NewtonResult` - результат оптимизации.
"""
function newtontemplate(
    f::Function,
    ∇f::Function,
    hess::Function,
    x0::AbstractVector;
    gtol::Real=1e-5,
    maxiter::Integer=200,
)
    x = float.(x0)  # копирование и приведение к флоат-числам

    # проверяем сходимость по норме-2 градиента
    g = ∇f(x)
    # ... && return NewtonResult(true, x, 0)

    for i in 1:maxiter
        # выбор направления d из модифицированного Гессиана Hᵐ
        # см. mcholesky!, LinearAlgebra.ldiv!
        # ...
        # d = ...

        # выбор шага вдоль d
        # α = ...
        # ... && return NewtonResult(false, x, i)  # α не найдено
        
        # совершаем шаг
        # x = ...

        # проверяем сходимость
        g = ∇f(x)
        # ... && return NewtonResult(true, x, i)
    end
    return NewtonResult(false, x, maxiter)
end
:::
```
